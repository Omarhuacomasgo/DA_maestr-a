---
title: "Análisis de datos – tesis de maestría"
author: "Omar Huaco"
date: last-modified     # se actualiza al compilar
lang: es
execute:
  warning: false        # suprime advertencias
  message: false        # suprime mensajes
  echo: true            # (opcional) muestra el código
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-overflow: scroll
    df-print: paged
    highlight-style: github
---


```{r}
#| label: setup
#| include: false

knitr::opts_knit$set(
  echo = TRUE,               # muestra el código
  warning = FALSE,           # no muestra advertencias
  message = FALSE,           # no muestra mensajes
  fig.width = 8,            # ancho de las figuras
  fig.height = 5,           # alto de las figuras
  fig.align = "center",    # alinea las figuras al centro
  dpi = 300)           # resolución de las figuras
  
```


# Resumen 

Este informe reconstruye el análisis de datos ejecutado para el proyecto de tesis

```{r, message = FALSE, results= 'hide'}

lapply(c("tidyverse", "readxl", "writexl", "modelsummary", "haven", "janitor", "labelled", "forcats"), library, 
       character.only = TRUE)

```

# Carga de datos y data wrangling

En esta sección se carga el conjunto de datos del Latinobarómetro 2023 y se realiza un preprocesamiento básico para su análisis.

```{r, message = FALSE, results = 'hide'}
#| label: load-data
#| include: true

lb2023 <- haven :: read_sav("E:\\Bases de datos\\latinobarometro_datasets\\lb2023.sav")
```


```{r, message = FALSE, results = 'hide'}
lb2023 <- lb2023 %>%  mutate(across(where(is.character), trimws)) %>% 
  rename_with(~ gsub("[._[:space:]]+", "", tolower(.x)))

```

La base de datos global contiene metadatos que describen las variables, como etiquetas y valores. Para explorar estos metadatos, se utiliza la función `look_for` del paquete `labelled`.

```{r, message = FALSE}
# 1. Exploración de los metadatos
lb2023_metadata <- look_for(lb2023) %>% as_tibble()
lb2023_metadata
```


```{r, message = FALSE}
# 2. Exporta a Excel con writexl
writexl::write_xlsx(x = lb2023, path = file.path("Exports", "Tables", "lb2023.xlsx"))
```

# Preparacion de datos

## Definición de listas de variables

Extraemos las variables de interés en listas que podemos editar posteriormente. En este caso, se extraen las variables dependientes e independientes que se utilizarán en el análisis.

Podemos crear una lista de varibles dependientes clasificandolas segun su rol en nuestros modelos. Por ejemplo,`principal_predictors` son las variables principales que se espera que tengan un efecto significativo en la variable dependiente, mientras que `sociodem_controls` y `other_controls` son variables de control que se incluirán para ajustar el modelo y controlar por posibles confusores.

Con `var_lists` se agrupan todas las variables en una lista para facilitar su manejo y edición posterior, luego con `all_of` se seleccionan las variables de la base de datos previamente almacenadas en `var_lists`.

```{r}
#| label: extract-data
#| include: true

# Variable dependiente
dep_vars <- c("p13std")  # Confianza en el Congreso

# Predictores principales (editables)
principal_predictors <- c(
  "p18sta",   # Problemas en la democracia
  "p18sti",   # La democracia soluciona problemas
  "p18ne",    # Poder judicial independiente
  "p54na",    # Tolerancia a la protesta
  "p11stgbsa",# Satisfacción con la democracia
  "p11stgbsb" # Satisfacción con la economía
)

sociodem_controls <- c(
  "edad",     # Edad 
  "sexo",
  "s1",       # Religión
  "s1a",      # Practica religiosa
  "s2",       # Clase social subjetiva
  "s7",       # Raza
  "s18a"      # Ocupación
)
# Otras variabels de control
other_controls <- c(
  "p16st",    # Eje izquierda–derecha
  "p17st",    # Cuán justa es la distribución de ingreso
  "reg",      # Región
  ## Variables asociadas al diseño muestral
  "ciudad",   # Ciudad
  "tamciud",  # Tamaño de la ciudad
  "idenpa",   # País
  "wt"    # Ponderación muestral
)

# Agrupar todo en una lista para fácil acceso
var_lists <- list(
  dep_vars             = dep_vars,
  principal_predictors = principal_predictors,
  sociodem_controls    = sociodem_controls,
  other_controls       = other_controls
)

# Seleccionar todas las variables relevantes
lb2023_a <- lb2023 %>% select(
    all_of(var_lists$dep_vars),
    all_of(var_lists$principal_predictors),
    all_of(var_lists$sociodem_controls),
    all_of(var_lists$other_controls))
```


```{r}
lb2023_a <- lb2023_a %>%
  mutate(
    edad = as.numeric(edad)  # conservar edad como numérica
  ) %>%
  mutate(
    across(
      .cols = where(haven::is.labelled),
      .fns  = ~ haven::as_factor(.x, levels = "labels")
    )
  )
```


# Procesamiento de niveles de variables categoricas

Generamos una lista del universo de categorías ordinales que utiliza el latinobarometro en sus encuestas. Nos aseguramos que R reconozca estas variables como factores ordenados.

```{r}
# Listas de niveles ordinales estándar
ord_foursteps <- c("Ninguna", "Poca", "Algo", "Mucha")
cs_subjective_ord <- c("Baja", "Media Baja", "Media", "Media Alta", "Alta")
justice_ord <- c("Muy injusta", "Injusta", "Justa", "Muy justa")
ord_satis     <- c("Nada satisfecho", "No muy satisfecho", "Mas bien satisfecho", "Muy satisfecho")
ord_agree     <- c("Muy en desacuerdo", "En desacuerdo", "De acuerdo", "Muy de acuerdo")
unknown_cat   <- c("No sabe", "No contesta", "No aplicable", "No preguntada", "No sabe / No contesta")
```

```{r}
# Recodificar todas las variables ordinales de manera unificada:
lb2023_a <- lb2023_a %>%
  mutate(
    across(
      .cols = where(is.factor),
      .fns = ~ {
        x_chr <- as.character(.x)
        # Marcar datos desconocidos como NA
        x_chr[x_chr %in% unknown_cat] <- NA
        # Identificar y aplicar niveles
        if (all(na.omit(x_chr) %in% ord_foursteps)) {
          factor(x_chr, levels = ord_foursteps, ordered = TRUE)
        } else if (all(na.omit(x_chr) %in% cs_subjective_ord)) {
          factor(x_chr, levels = cs_subjective_ord, ordered = TRUE)
        } else if (all(na.omit(x_chr) %in% justice_ord)) {
          factor(x_chr, levels = justice_ord, ordered = TRUE)
        } else if (all(na.omit(x_chr) %in% ord_satis)) {
          factor(x_chr, levels = ord_satis, ordered = TRUE)
        } else if (all(na.omit(x_chr) %in% ord_agree)) {
          factor(x_chr, levels = ord_agree, ordered = TRUE)
        } else {
          # Devolver original si no coincide con ninguna escala
          .x
        }
      }
    )
  )
```

Procesamos por separado algunas variables especiales. 

```{r, warning=FALSE, message=FALSE}
# Recodificación específica de p16st (eje izquierda–derecha) en un bloque aparte
lb2023_a <- lb2023_a %>%
  mutate(
    p16st = case_when(
      grepl("^00", p16st)                 ~ 0L,
      grepl("^10", p16st)                 ~ 10L,
      p16st %in% as.character(1:9)       ~ as.integer(as.character(p16st)),
      TRUE                                ~ NA_integer_
    )
  )

```


Finalmente, filtramos las observaciones a Perú

```{r}
lb2023_pe <- lb2023_a %>% filter(idenpa == " Peru") 
```


# Analisis de valores perdidos

## Estadísticos de resumen 

En esta sección, se analiza univariadamente cada variable del dataset. Comprobamos que las variables categóricas tengan los niveles correctos y que las variables numéricas estén en el formato adecuado. 
Se identifican los valores perdidos y se proporciona un resumen de las variables.

```{r}
library(skimr)
skimr :: skim(lb2023_pe) %>% as_tibble()
```

```{r}
library(naniar)
naniar :: vis_miss(lb2023_pe)
```
# Imputación rapida
```{r, warning=FALSE, message=FALSE}
library(mice)
# 1. Identificar columnas con NA
cols_na <- names(which(colSums(is.na(lb2023_pe)) > 0))
```

Clasificamos las variables según su tipo para aplicar métodos de imputación adecuados. Las variables numéricas, ordinales y categóricas se manejan de manera diferente en el proceso de imputación.

```{r}
# 2. Clasificar variables
numeric_vars    <- cols_na[sapply(lb2023_pe[cols_na], is.numeric)]
ordinal_vars    <- cols_na[sapply(lb2023_pe[cols_na], is.ordered)]
categorical_vars <- cols_na[sapply(lb2023_pe[cols_na], function(x) is.factor(x) && !is.ordered(x))]
```

Le decimos a R que en meth_vec vamos a definir los métodos de imputación para cada tipo de variable. Utilizamos `make.method` del paquete `mice` para crear un vector de métodos de imputación. Luego, asignamos el método adecuado a cada tipo de variable.

```{r}
# 3. Definir métodos de imputación para MICE
meth_vec <- make.method(lb2023_pe[cols_na])

meth_vec[numeric_vars]    <- "pmm"     # Predictive Mean Matching para numéricas
meth_vec[ordinal_vars]    <- "polr"    # Proportional Odds Logistic Regression para ordinales
meth_vec[categorical_vars] <- "polyreg" # Regresión polinómica para nominales
```

Ejecutamos el proceso de imputación multivariada utilizando la función `mice`. Especificamos el número de imputaciones (`m = 10`) y un valor de semilla para la reproducibilidad (`seed = 123`). La opción `printFlag = FALSE` suprime la salida de progreso.

```{r}
# 4. Ejecutar imputación multivariada
imp_full <- mice(lb2023_pe[cols_na], method = meth_vec, m = 10, seed = 123, printFlag = FALSE)

# 5. Completar base con la primera imputación (puede cambiarse a pool)
lb2023_pe_imp <- lb2023_pe
lb2023_pe_imp[cols_na] <- complete(imp_full, 1)

# 6. Resumen rápido de columnas imputadas
tibble(
  tipo        = c("Numéricas", "Ordinales", "Nominales"),
  n_variables = c(length(numeric_vars), length(ordinal_vars), length(categorical_vars))
)

```

```{r}
lb2023_pe <- lb2023_pe_imp
skimr :: skim(lb2023_pe) %>% as_tibble()
```
# Modelamiento

## Las regresiones logísticas ordinales

Las regresiones logísticas ordinales resultan idóneas cuando la variable de interés toma valores ordenados (por ejemplo niveles de satisfacción o confianza) sin que la distancia entre ellos sea necesariamente constante. En tu caso, la confianza en el Congreso (p13std) se modela en función de un conjunto de predictores 𝑥, capturando el carácter ordinal de las categorías. Matemáticamente, para una variable 𝑌con 𝐾 niveles (1,2,…,𝐾) el modelo de probabilidades acumuladas se expresa como:

Para cada \(k=1,\dots,K-1\):

$$
\Pr(Y \le k \mid x) = \frac{1}{1 + \exp\bigl(-(\alpha_k - x^\top\beta)\bigr)}.
$$

Equivalentemente,

$$
\log\frac{\Pr(Y \le k \mid x)}{\Pr(Y > k \mid x)}
=\alpha_k - x^\top\beta.
$$


## Especificacion de modelos

```{r, message=FALSE, warning=FALSE}

library(MASS)   # polr()
library(broom)  # tidy / glance
library(purrr)  # map / imap

# Modelo 1: Sólo predictores principales
mod_ord1 <- polr(
  p13std ~ p18sta + p18sti + p18ne + p54na + p11stgbsa + p11stgbsb,
  data    = lb2023_pe,
  weights = wt,
  Hess    = TRUE
)

# Modelo 2: + controles sociodemográficos
mod_ord2 <- polr(
  p13std ~ p18sta + p18sti + p18ne + p54na + p11stgbsa + p11stgbsb + edad + sexo + s2,
  data    = lb2023_pe,
  weights = wt,
  Hess    = TRUE
)

# Modelo 3: + variables político-ideológicas y de contexto
mod_ord3 <- polr(p13std ~ p18sta + p18sti + p18ne + p54na + p11stgbsa + p11stgbsb + 
                   edad + sexo + s2 + 
                   p16st + p17st,
  data    = lb2023_pe,
  weights = wt,
  Hess    = TRUE
)

```


```{r}
tab_ord1 <- tidy(mod_ord1, conf.int = TRUE) %>%
  mutate(
    OR       = exp(estimate),      # convierte log-odds en odds ratio
    p.value  = 2 * (1 - pnorm(abs(statistic))),
    signif   = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ ""
    )
  ) %>% dplyr::  select(term, estimate, OR, std.error, statistic, p.value, signif, conf.low, conf.high)

tab_ord2 <- tidy(mod_ord2, conf.int = TRUE) %>%
  mutate(
    OR       = exp(estimate),      # convierte log-odds en odds ratio
    p.value  = 2 * (1 - pnorm(abs(statistic))),
    signif   = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ ""
    )
  ) %>% dplyr ::  select(term, estimate, OR, std.error, statistic, p.value, signif, conf.low, conf.high)

tab_ord3 <- tidy(mod_ord3, conf.int = TRUE) %>%
  mutate(
    OR       = exp(estimate),      # convierte log-odds en odds ratio
    p.value  = 2 * (1 - pnorm(abs(statistic))),
    signif   = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ ""
    )
  ) %>% dplyr:: select(term, estimate, OR, std.error, statistic, p.value, signif, conf.low, conf.high)

#tab_ordx %>% as_tibble()
```


`
```{r}
# Mostrar tabla con gt (Quarto friendly)
library(gt)
tab_ord1 %>%
  gt() %>%
  tab_header(
    title = md("**Tabla de resultados mod_ord1**"),
    subtitle = md("Coeficientes (log-odds), OR, p-values y significancia")
  ) %>%
  fmt_number(
    columns = vars(estimate, OR, std.error, statistic, p.value, conf.low, conf.high),
    decimals = 2
  ) %>%
  cols_label(
    term       = "Predictor",
    estimate   = "Coef",
    OR         = "OR",
    std.error  = "SE",
    statistic  = "z",
    p.value    = "p",
    signif     = "Signif",
    conf.low   = "IC 2.5%",
    conf.high  = "IC 97.5%"
  ) %>%
  tab_source_note(md("Signif.: * p<0.05; ** p<0.01; *** p<0.001"))

```


```{r}
# Mostrar tabla con gt (Quarto friendly)
library(gt)
tab_ord2 %>%
  gt() %>%
  tab_header(
    title = md("**Tabla de resultados mod_ord2**"),
    subtitle = md("Coeficientes (log-odds), OR, p-values y significancia")
  ) %>%
  fmt_number(
    columns = vars(estimate, OR, std.error, statistic, p.value, conf.low, conf.high),
    decimals = 2
  ) %>%
  cols_label(
    term       = "Predictor",
    estimate   = "Coef",
    OR         = "OR",
    std.error  = "SE",
    statistic  = "z",
    p.value    = "p",
    signif     = "Signif",
    conf.low   = "IC 2.5%",
    conf.high  = "IC 97.5%"
  ) %>%
  tab_source_note(md("Signif.: * p<0.05; ** p<0.01; *** p<0.001"))

```


```{r}
# Mostrar tabla con gt (Quarto friendly)
library(gt)
tab_ord3 %>%
  gt() %>%
  tab_header(
    title = md("**Tabla de resultados mod_ord1**"),
    subtitle = md("Coeficientes (log-odds), OR, p-values y significancia")
  ) %>%
  fmt_number(
    columns = vars(estimate, OR, std.error, statistic, p.value, conf.low, conf.high),
    decimals = 2
  ) %>%
  cols_label(
    term       = "Predictor",
    estimate   = "Coef",
    OR         = "OR",
    std.error  = "SE",
    statistic  = "z",
    p.value    = "p",
    signif     = "Signif",
    conf.low   = "IC 2.5%",
    conf.high  = "IC 97.5%"
  ) %>%
  tab_source_note(md("Signif.: * p<0.05; ** p<0.01; *** p<0.001"))

```


Si deseas ver las tablas en formato apa, puedes usar el siguiente codigo
```{r}
library(modelsummary)

# Construir lista de modelos con etiquetas
models_list <- list(
  "Modelo 1" = mod_ord1,
  "Modelo 2" = mod_ord2,
  "Modelo 3" = mod_ord3
)

# Generar tabla APA
modelsummary(
  models_list,
  output = "kableExtra",    # Salida en HTML para Quarto
  stars  = TRUE,             # Estrellas de significancia
  fmt    = 2,                # Dos decimales
  style  = "apa",          # Formato APA
  coef_omit = "(Intercept)", # Omitir interceptos si se desea
  title  = "Tabla APA de modelos de regresión ordinal"
) %>%
  kableExtra::kable_styling(full_width = FALSE)
```
```

